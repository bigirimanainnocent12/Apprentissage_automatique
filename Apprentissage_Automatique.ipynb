{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNl739r5PWuI1mbC2z5vzgA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bigirimanainnocent12/Apprentissage_automatique/blob/main/Apprentissage_Automatique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **projet: Cours apprentissage automatique**"
      ],
      "metadata": {
        "id": "O5_Mq66Npzqx"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings # Ignorer tous les avertissements\n",
        "warnings.filterwarnings(\"ignore\") # Ignorer les avertissements spécifiques\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "evI4fWUSw5x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***1. Chargement et préparation de données réelles***\n"
      ],
      "metadata": {
        "id": "Hdz01mjuN1ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Télécharger la dernière version\n",
        "chemin = kagglehub.dataset_download(\"thehapyone/uci-online-news-popularity-data-set\")\n",
        "\n",
        "print(\"Path to dataset files:\", chemin)\n",
        "\n",
        "# Trouver le fichier CSV dans le répertoire téléchargé\n",
        "for filename in os.listdir(chemin):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        csv_file_path = os.path.join(chemin, filename)\n",
        "        break  # Arrêter après avoir trouvé le premier fichier CSV\n",
        "\n",
        "# Lire le fichier CSV\n",
        "DF = pd.read_csv(csv_file_path)\n",
        "DF.head()\n"
      ],
      "metadata": {
        "id": "hmlU_JBcPC6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Déscription de la base de données***\n",
        "\n",
        "La base de données synthétise une variété de caractéristiques concernant les articles diffusés par Mashable sur une durée de deux ans environ. cet ensemble comprend 61 variables:\n",
        "\n",
        "1. url : URL de l'article\n",
        "2. timedelta : Jours entre la publication de l'article et l'acquisition du jeu de données\n",
        "\n",
        "3. n_tokens_title : Nombre de mots dans le titre (Quantitative)\n",
        "4. n_tokens_content : Nombre de mots dans le contenu (Quantitative)\n",
        "5. n_unique_tokens : Taux de mots uniques dans le contenu (Quantitative)\n",
        "6. n_non_stop_words : Taux de mots non-stop dans le contenu (Quantitative)\n",
        "7. n_non_stop_unique_tokens : Taux de mots uniques non-stop dans le contenu (Quantitative)\n",
        "8. num_hrefs : Nombre de liens (Quantitative)\n",
        "9. num_self_hrefs : Nombre de liens vers d'autres articles publiés par Mashable (Quantitative)\n",
        "10. num_imgs : Nombre d'images (Quantitative)\n",
        "11. num_videos : Nombre de vidéos (Quantitative)\n",
        "12. average_token_length : Longueur moyenne des mots dans le contenu (Quantitative)\n",
        "13. num_keywords : Nombre de mots-clés dans les métadonnées (Quantitative)\n",
        "14. data_channel_is_lifestyle : Le canal de données est-il  Lifestyle ? (Binaire)\n",
        "15. data_channel_is_entertainment : le canal de données est-il  Divertissement? (Binaire)\n",
        "16. data_channel_is_bus : Le canal de données est-il professionnel  ? (Binaire)\n",
        "17. data_channel_is_socmed : Le canal de données est-il un  réseau social? (Binaire)\n",
        "18. data_channel_is_tech : Le canal de données est-il  Tech ? (Binaire)\n",
        "19. data_channel_is_world : Le canal de données est-il World ? (Binaire)\n",
        "20. kw_min_min : pire mot-clé (partages min.) (Quantitative)\n",
        "21. kw_max_min : pire mot-clé (partages max.) (Quantitative)\n",
        "22. kw_avg_min : pire mot-clé (partages moyens) (Quantitative)\n",
        "23. kw_min_max : Meilleur mot-clé (min. de partages) (Quantitative)\n",
        "24. kw_max_max : Meilleur mot-clé (partages max.) (Quantitative)\n",
        "25. kw_avg_max : Meilleur mot-clé (partages moyens) (Quantitative)\n",
        "26. kw_min_avg : mot-clé moyen (partages min.) (Quantitative)\n",
        "27. kw_max_avg : mot-clé moyen (partages max.) (Quantitative)\n",
        "28. kw_avg_avg : mot-clé moyen (partages moyens) (Quantitative)\n",
        "29. self_reference_min_shares : nombre minimum de parts d'articles référencés dans Mashable (Quantitative)\n",
        "30. self_reference_max_shares : nombre maximal de parts d'articles référencés dans Mashable (Quantitative)\n",
        "31. self_reference_avg_sharess : Moyenne des parts d'articles référencés .dans Mashable (Quantitative)\n",
        "32. weekday_is_monday : L’article a-t-il été publié un lundi ? (Binaire)\n",
        "33. weekday_is_tuesday : L'article a-t-il été publié un mardi ? (Binaire)\n",
        "34. weekday_is_wednesday : L'article a-t-il été publié un mercredi ? (Binaire)\n",
        "35. weekday_is_thursday : L'article a-t-il été publié un jeudi ? (Binaire)\n",
        "36. weekday_is_friday : L'article a-t-il été publié un vendredi ? (Binaire)\n",
        "37. weekday_is_saturday : L'article a-t-il été publié un samedi ? (Binaire)\n",
        "38. weekday_is_sunday : L'article a-t-il été publié un dimanche ? (Binaire)\n",
        "39. is_weekend : L'article a-t-il été publié le week-end ? (Binaire)\n",
        "40. LDA_00 : Proximité avec le sujet LDA 0 (Quantitative)\n",
        "41. LDA_01 : Proximité avec le thème LDA 1 (Quantitative)\n",
        "42. LDA_02 : Proximité avec le thème LDA 2 (Quantitative)\n",
        "43. LDA_03 : Proximité avec le thème 3 de LDA (Quantitative)\n",
        "44. LDA_04 : Proximité avec le thème LDA 4 (Quantitative)\n",
        "45. global_subjectivity : subjectivité du texte (Quantitative)\n",
        "46. global_sentiment_polarity : Polarité des sentiments du texte (Quantitative)\n",
        "47. global_rate_positive_words : Taux de mots positifs dans le contenu (Quantitative)\n",
        "48. global_rate_negative_words : Taux de mots négatifs dans le contenu (Quantitative)\n",
        "49. rate_positive_words : Taux de mots positifs parmi les jetons non neutres (Quantitative)\n",
        "50. rate_negative_words : Taux de mots négatifs parmi les jetons non neutres (Quantitative)\n",
        "51. avg_positive_polarity : Polarité moyenne des mots positifs. (Quantitative)\n",
        "52. min_positive_polarity : Polarité minimale des mots positifs (Quantitative)\n",
        "53. max_positive_polarity : Polarité max. des mots positifs (Quantitative)\n",
        "54. avg_negative_polarity : Polarité moyenne des mots négatifs (Quantitative)\n",
        "55. min_negative_polarity : Polarité minimale des mots négatifs (Quantitative)\n",
        "56. max_negative_polarity : Polarité max. des mots négatifs (Quantitative)\n",
        "57. title_subjectivity : Subjectivité du titre (Quantitative)\n",
        "58. title_sentiment_polarity : Polarité du titre (Quantitative)\n",
        "59. abs_title_subjectivity : Niveau de subjectivité absolue (Quantitative)\n",
        "60. abs_title_sentiment_polarity : Niveau de polarité absolu (Quantitative)\n",
        "61. actions : Nombre d'actions (cible) (Quantitative la réponse)\n",
        "\n",
        "\n",
        "# Remarque\n",
        "\n",
        "Nous allons suprimer les variables url et timedelta car ils n'apportent aucune information sur la variable cible \"partage\"."
      ],
      "metadata": {
        "id": "WxRWRsikl2WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Df=DF.drop([\"url\",\" timedelta\"],axis=1)"
      ],
      "metadata": {
        "id": "2LgVbdYDtB07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Détails des informations de la base de données:\n"
      ],
      "metadata": {
        "id": "aLFzRYwutuWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Df.info()"
      ],
      "metadata": {
        "id": "F2JmWxKyeGYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remarque**\n",
        "\n",
        "Premièrement, on va changer le type des variables binaires car dans la base de données, elles sont de type \"float64\"."
      ],
      "metadata": {
        "id": "x7j4p9hQt6Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variables_qualitative=Df[[' weekday_is_monday',' weekday_is_tuesday',' weekday_is_wednesday',' weekday_is_thursday',' weekday_is_friday',' weekday_is_saturday',' weekday_is_sunday',\n",
        "                        ' is_weekend',' data_channel_is_entertainment',' data_channel_is_bus',' data_channel_is_socmed',' data_channel_is_tech',\n",
        "                         ' data_channel_is_world',' data_channel_is_lifestyle' ]].astype('bool')\n",
        "\n",
        "variables_dicretes=Df[[' n_tokens_title',' n_tokens_content',' num_hrefs',' num_self_hrefs',' num_imgs',' num_videos',' num_keywords']].astype('int64')\n",
        "\n",
        "variables_quantitative = Df[Df.columns.difference(variables_qualitative.columns.union(variables_dicretes.columns))]\n",
        "\n",
        "data=pd.concat([variables_quantitative,variables_qualitative,variables_dicretes],axis=1)"
      ],
      "metadata": {
        "id": "tTW9C_vJut0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vérifications des Valeurs manquantées**\n",
        "\n",
        "Nous remarquons que dans notre base de données, il n'y a pas des données manquantées."
      ],
      "metadata": {
        "id": "I2GCHA4ntSCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "HZJ3Xh7PoHb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vérifications des Valeurs dupliquées**\n",
        "\n",
        "Nous remarquons également que dans la base de données, il n'y a pas des articles dupliqués"
      ],
      "metadata": {
        "id": "rzvdNg7RtJ3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "JECyIAmQsxYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **La variable cible et ses liens avec les variables explicatives considérées une à une.**"
      ],
      "metadata": {
        "id": "_punSAKuleNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistiques descriptives pour la variable cible\n",
        "\n",
        "D'après les résultats ci dessous;\n",
        "\n",
        "- Il y a 39 644 valeurs dans la colonne shares. Cela signifie qu'il n'y a pas de valeurs manquantes dans cette variable.\n",
        "- La moyenne est de 3 395.38. Cela indique que, en moyenne, les articles reçoivent environ 3 395 partages. Cependant, cette moyenne peut être influencée par des valeurs extrêmes (comme la valeur maximale de 843 300).\n",
        "- L'écart type est de 11 626.95, ce qui est très élevé. Cela reflète une grande dispersion des données autour de la moyenne. En d'autres termes, les articles ont des nombres de partages très variables, certains étant très populaires (avec des partages très élevés) et d'autres beaucoup moins.\n",
        "- Minimum : La plus petite valeur est 1, ce qui signifie qu'au moins un article a reçu un seul partage.\n",
        "- Maximum : La plus grande valeur est 843 300, ce qui indique qu'un article a été extrêmement populaire. Cette valeur est très éloignée de la moyenne, ce qui pourrait suggérer un outlier (valeur aberrante).\n",
        "- Percentiles (25%, 50%, 75%):\n",
        "\n",
        "* 25% (premier quartile) : 25 % des articles ont 946 partages ou moins.\n",
        "\n",
        "* 50% (médiane) : La moitié des articles ont 1 400 partages ou moins. Cela suggère que la répartition est asymétrique (car la médiane est bien inférieure à la moyenne).\n",
        "* 75% (troisième quartile) : 75 % des articles ont 2 800 partages ou moins.\n",
        "\n",
        "La différence entre le 75e percentile (2 800) et la moyenne (3 395.38) indique que la moyenne est tirée vers le haut par des valeurs très élevées (comme la valeur maximale de 843 300).\n",
        "\n",
        "# Remarque\n",
        "\n",
        "La distribution des partages est asymétrique à droite, ce qui signifie qu'une petite proportion d'articles obtient un nombre de partages exceptionnellement élevé, tandis que la majorité reçoit moins de partages.\n",
        "\n",
        "Il y a une grande variabilité dans les données (écart type élevé).\n",
        "La médiane (1 400) étant bien inférieure à la moyenne (3 395.38), il est probable que les données contiennent plusieurs outliers ou articles très populaires qui tirent la moyenne vers le haut."
      ],
      "metadata": {
        "id": "bY8YlKfhn7tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\" shares\"].describe()"
      ],
      "metadata": {
        "id": "h1mKlHLBlkd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Les liens entre la variable cible et ses variables explicatives considérées une à une***\n",
        "\n",
        "Pour montrer une relation linéaire entre deux variables quantitatives, nous allons utiliser le coefficient de correlation de pearson.\n"
      ],
      "metadata": {
        "id": "SantQPzT1PxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corrélation entre la variable réponse et ses variables explicatives quantitatives\n",
        "\n",
        "D'après la figure ci-desous, nous remarquons que:\n",
        "\n",
        "- Variables importantes : Les variables avec les plus fortes corrélations positives (par exemple, kw_avg_avg, kw_max_avg) devraient être étudiées de près, car elles sont les meilleurs prédicteurs linéaires de shares.\n",
        "\n",
        "- Corrélation globale faible : Les faibles coefficients de corrélation suggèrent que les relations linéaires entre les variables explicatives et partage sont limitées. Des méthodes plus avancées (comme les modèles non linéaires) pourraient mieux capturer les relations complexes.\n",
        "\n",
        "- Impact négatif : Les variables avec des corrélations négatives significatives (par exemple, LDA_02, data_channel_is_world) pourraient refléter des caractéristiques qui rendent un article moins populaire."
      ],
      "metadata": {
        "id": "YI8iAXp95gKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as pe\n",
        "\n",
        "fig = pe.bar(\n",
        "    x=pd.concat([variables_quantitative,variables_dicretes],axis=1).corr()[\" shares\"].sort_values(ascending=False)[1:].index,\n",
        "    y=pd.concat([variables_quantitative,variables_dicretes],axis=1).corr()[\" shares\"].sort_values(ascending=False)[1:],\n",
        "    title=\"Corrélation entre la variable réponse et ses variables explicatives quantitatives\"\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_tickangle=90,\n",
        "    yaxis_title=\"Coefficient de corrélation\"\n",
        ")\n",
        "\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "34KD5vHZihwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "quantitative_vars = pd.concat([variables_quantitative,variables_dicretes],axis=1)\n",
        "\n",
        "target_variable = ' shares'\n",
        "\n",
        "plt.figure(figsize=(12, 10),dpi=100)\n",
        "\n",
        "sns.heatmap(quantitative_vars.corr()[[target_variable]].sort_values(by=[target_variable], ascending=False), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Quantitative Variables')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9rUuGqwfvqui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test : Coefficient de corrélation de Pearson\n",
        "\n",
        "Pour savoir si ces coefficients de corélation sont statistique différent de zero, nous allons formuler les hypothéses suivantes:\n",
        "\n",
        "$H_0$: La variable explicative n'a pas d'effet significatif sur la popularité de l'article.\n",
        "\n",
        "$H_1$: La variable explicative a une effet significatif sur la popularité de l'article.\n",
        "\n"
      ],
      "metadata": {
        "id": "CLKgryIfQ1y1"
      }
    },
    {
      "source": [
        " !pip install pingouin\n",
        "import pingouin as pg\n",
        "\n",
        "cols_to_correlate = pd.concat([variables_quantitative, variables_dicretes], axis=1).columns\n",
        "\n",
        "\n",
        "correlation_results = {}\n",
        "\n",
        "\n",
        "for col in cols_to_correlate:\n",
        "    result = pg.corr(data[col], data[' shares'])  # Pass individual columns\n",
        "    correlation_results[col] = {\n",
        "        'r': result['r'].values[0],  # Extract correlation coefficient (r)\n",
        "        'p-val': result['p-val'].values[0]  # Extract p-value\n",
        "    }\n",
        "\n",
        "\n",
        "correlation_df = pd.DataFrame(correlation_results).T\n",
        "\n",
        "correlation_df[correlation_df[\"p-val\"]< 0.05].sort_values(by=\"r\",ascending=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tOEESfzeSHgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# les liens entre la variable cible et ses variables explicatives qualitatives considérées une à une"
      ],
      "metadata": {
        "id": "NNXdLhOU9m5y"
      }
    },
    {
      "source": [
        "from  scipy import stats\n",
        "cols_to_correlate = variables_qualitative.columns\n",
        "\n",
        "test_student = {}\n",
        "\n",
        "for col in cols_to_correlate:\n",
        "    test = stats.mannwhitneyu(data[' shares'].where(data[col]==True).dropna(), data[' shares'].where(data[col]==False).dropna(),alternative='two-sided')\n",
        "    test_student[col] = {\n",
        "        'statistic': test.statistic,\n",
        "        'p-val': test.pvalue\n",
        "    }\n",
        "\n",
        "test_resulat = pd.DataFrame(test_student).T\n",
        "test_resulat[test_resulat[\"p-val\"]< 0.05].sort_values(by=\"p-val\",ascending=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mpg2r-nudGnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***2. Prédiction***\n",
        "\n",
        "Comme les variables explicatives ne sont pas corrélées de façon linéaire avec la variable réponse (voire la figure des corrélations) i.e les  coefficient de corrélations sont faibles ou quasi- nulles. nous allons effectuer la discrétisation de la variable réponse en deux catégorie. La valeur de référence est la mediane car elle n'est pas sensible aux valeurs aberrantes. L'objectif est de savoir si un article a été populaire ou pas. la nouvelle variable obtenue sera code 0 pour dire que l'article n'a pas été populaire et 1 pour dire que l'article est populaire."
      ],
      "metadata": {
        "id": "DZLVl2GSF1-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def z(share):\n",
        "\n",
        "  return share.apply(lambda x: '0' if x <=1400 else '1').astype(\"object\")\n",
        "\n",
        "data[\"Popularite\"]= pd.DataFrame(z(data[' shares']))\n",
        "data.value_counts(\"Popularite\")"
      ],
      "metadata": {
        "id": "LAOUfvRLsRv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remarque**:\n",
        "\n",
        "D'après le tableau obtenu, on remarque que 20082 articles n'ont pas été populaire contre 19562 articles qui ont été populaire.\n",
        "\n",
        "\n",
        "La nouvelle variable est binaire, code 0 et 1. pour construire un modèle pour expliquer cette variable \"populalité\", nous allons utiliser les méthodes de classificationns comme les forêts aleatoire, les KNN et la regression logistique qui sont des methodes robustes pour traiter ce genre de problème.\n",
        "\n",
        "\n",
        "1. Forêts aléatoires\n",
        "\n",
        "Les forêts aléatoires (Random Forests) sont une technique d'apprentissage automatique basée sur les arbres de décision. Elles fonctionnent en créant une multitude d'arbres de décision sur divers sous-ensembles du jeu de données et en utilisant la moyenne des prédictions de chaque arbre pour obtenir une meilleure précision. Les forêts aléatoires sont particulièrement utiles pour la classification et la régression et sont robustes face au surapprentissage (overfitting).\n",
        "- Avantages:\n",
        "1. Les forêts aléatoires réduisent le risque de surapprentissage par rapport aux arbres de décision individuels en agrégeant les prédictions de plusieurs arbres.\n",
        "2. Les forêts aléatoires répondent aux contraintes des arbres de décision en réduisant la sensibilité au bruit en combinant plusieurs arbres, ce qui stabilise les prédictions.\n",
        "\n",
        "-  inconvénients:\n",
        "\n",
        "\n",
        "2. K-Nearest Neighbors (KNN)\n",
        "\n",
        "Le KNN est un algorithme d'apprentissage supervisé utilisé à la fois pour la classification et la régression. Il fonctionne en trouvant les K exemples les plus proches (voisins) dans le jeu de données pour effectuer une prédiction pour un nouvel exemple. La proximité est mesurée à l'aide d'une distance, comme la distance euclidienne. KNN est simple à comprendre et à mettre en œuvre, mais peut être lent avec de grands jeux de données.\n",
        "\n",
        "- Avantages:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "-  inconvénients:\n",
        "\n",
        "3. Régression logistique\n",
        "\n",
        "La régression logistique est une méthode statistique pour la classification binaire, bien qu'elle puisse être étendue à des cas multiclasse. Elle modélise la probabilité qu'un exemple appartienne à une certaine classe en utilisant une fonction logistique. Contrairement à la régression linéaire, qui prédit des valeurs continues, la régression logistique prédit des probabilités et applique un seuil pour assigner l'exemple à une classe particulière.\n",
        "\n",
        "- Avantages:\n",
        "\n",
        "\n",
        "\n",
        "-  inconvénients:"
      ],
      "metadata": {
        "id": "XLlwdredMGrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Trois méthodes de prédiction vues en cours***"
      ],
      "metadata": {
        "id": "iBaLzBCQ7eAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **a. aucune réduction des dimensions**\n"
      ],
      "metadata": {
        "id": "28mFFWk2DvFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=data.drop([\" shares\",\"Popularite\"],axis=1)\n",
        "Y=data[\"Popularite\"]"
      ],
      "metadata": {
        "id": "nHZhdHhIhELa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "OOsA91TyQGDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. RandomForestClassifie**"
      ],
      "metadata": {
        "id": "TBaZhiq484Iu"
      }
    },
    {
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('MISE A L ECHELLE', RobustScaler(), X.select_dtypes(include=['int64','float64']).columns),\n",
        "        ('BINAIRE', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns)\n",
        "])\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "model = make_pipeline(\n",
        "    preprocessor,\n",
        "    RandomForestClassifier()\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'randomforestclassifier__n_estimators': [100,150,200,250]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LE1djPNoye8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. KNeighborsClassifier**"
      ],
      "metadata": {
        "id": "qCr5vd-RutvH"
      }
    },
    {
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('MISE A L ECHELLE', RobustScaler(), X.select_dtypes(include=['int64','float64']).columns),\n",
        "        ('BINAIRE', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns)\n",
        "])\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model_1 = make_pipeline(\n",
        "    preprocessor,\n",
        "    KNeighborsClassifier()\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid_1 = {\n",
        "    'kneighborsclassifier__n_neighbors': [100,150,200,250]\n",
        "}\n",
        "\n",
        "grid_1 = GridSearchCV(model_1, param_grid=param_grid_1, cv=5, scoring='accuracy')\n",
        "grid_1.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1NoSqE--jNck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machines à vecteurs de support (SVM)**"
      ],
      "metadata": {
        "id": "LZW6NAVVqHwu"
      }
    },
    {
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('MISE A L ECHELLE', RobustScaler(), X.select_dtypes(include=['int64','float64']).columns),\n",
        "        ('BINAIRE', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns)\n",
        "])\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model_2 = make_pipeline(\n",
        "    preprocessor,\n",
        "    SVC()\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model_2.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uIadrPHuLhBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Réduction par la méthode ACP**"
      ],
      "metadata": {
        "id": "RO9G5GHLlYII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RandomForestClassifier**"
      ],
      "metadata": {
        "id": "69WKtK9DFNUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor_PCA = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('MISE A L ECHELLE', RobustScaler(), X.select_dtypes(include=['int64','float64']).columns),\n",
        "     ('binary', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns),\n",
        "         ('REDUCTIONS_DES_DIMENSIONS', PCA(n_components=30) , X.select_dtypes(include=['int64','float64',\"bool\"]).columns)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model_1_PCA = make_pipeline(\n",
        "    preprocessor_PCA,\n",
        "    RandomForestClassifier()\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid_1_PCA = {\n",
        "    'randomforestclassifier__n_estimators': [100,150,200,250]\n",
        "}\n",
        "grid_1_PCA= GridSearchCV(model_1_PCA, param_grid=param_grid_1_PCA, cv=5, scoring='accuracy')\n",
        "grid_1_PCA.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "BTl2KkhLeehg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KNeighborsClassifier**"
      ],
      "metadata": {
        "id": "O1cRfgqiGYlY"
      }
    },
    {
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor_1_PCA = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scaling', RobustScaler(), X.select_dtypes(include=['int64', 'float64']).columns),\n",
        "        ('binary', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns),\n",
        "        ('Reduction', PCA(n_components=30), X.select_dtypes(include=['int64', 'float64', \"bool\"]).columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model_1_PCA = make_pipeline(\n",
        "    preprocessor_1_PCA,\n",
        "    KNeighborsClassifier()\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid_1_PCA = {\n",
        "    'kneighborsclassifier__n_neighbors': [100, 150, 200, 250]\n",
        "}\n",
        "\n",
        "grid_1_PCA = GridSearchCV(model_1_PCA, param_grid=param_grid_1_PCA, cv=5, scoring='accuracy')\n",
        "grid_1_PCA.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bL7J6CjoUsye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machines à vecteurs de support (SVM)**"
      ],
      "metadata": {
        "id": "wGkVSXGVgvpu"
      }
    },
    {
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor_2_PCA = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scaling', RobustScaler(), X.select_dtypes(include=['int64', 'float64']).columns),\n",
        "        ('binary', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns),\n",
        "        ('Reduction', PCA(n_components=30), X.select_dtypes(include=['int64', 'float64', \"bool\"]).columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model_2_PCA = make_pipeline(\n",
        "    preprocessor_2_PCA,\n",
        "    SVC()\n",
        ")\n",
        "model_2_PCA.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aEYZbUsF7N4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Réduction UMAP**"
      ],
      "metadata": {
        "id": "l0cx4dnt_UxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RandomForestClassifier**"
      ],
      "metadata": {
        "id": "i57pDnMdAdmG"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import umap\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor_UMAP = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('MISE A L ECHELLE', RobustScaler(), X.select_dtypes(include=['int64','float64']).columns),\n",
        "     ('binary', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns),\n",
        "     ('REDUCTIONS_DES_DIMENSIONS', umap.UMAP(n_neighbors=30, min_dist=0.1,n_components=2, random_state=42) , X.select_dtypes(include=['int64','float64',\"bool\"]).columns)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model_1_UMAP= make_pipeline(\n",
        "    preprocessor_UMAP,\n",
        "    RandomForestClassifier()\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid_1_UMAP = {\n",
        "    'randomforestclassifier__n_estimators': [100,150,200,250]\n",
        "}\n",
        "grid_1_UMAP= GridSearchCV(model_1_UMAP, param_grid=param_grid_1_UMAP, cv=5, scoring='accuracy')\n",
        "grid_1_UMAP.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3tUYpVV9_a-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KNeighborsClassifier**"
      ],
      "metadata": {
        "id": "riCmCnumCgWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor_UMAP = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scaling', RobustScaler(), X.select_dtypes(include=['int64', 'float64']).columns),\n",
        "        ('binary', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns),\n",
        "        ('REDUCTIONS_DES_DIMENSIONS', umap.UMAP(n_neighbors=30, min_dist=0.1,n_components=2, random_state=42) , X.select_dtypes(include=['int64','float64',\"bool\"]).columns)\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model_1UMAP = make_pipeline(\n",
        "    preprocessor_UMAP,\n",
        "    KNeighborsClassifier()\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_gridUMAP = {\n",
        "    'kneighborsclassifier__n_neighbors': [100, 150, 200, 250]\n",
        "}\n",
        "\n",
        "grid_UMAP = GridSearchCV(model_1UMAP, param_grid=param_gridUMAP, cv=5, scoring='accuracy')\n",
        "grid_UMAP.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "lr8V8pvXCeTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Machines à vecteurs de support (SVM)***"
      ],
      "metadata": {
        "id": "kzog1X4AIPKJ"
      }
    },
    {
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "import umap\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor_UMAP = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scaling', RobustScaler(), X.select_dtypes(include=['int64', 'float64']).columns),\n",
        "        ('binary', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns),\n",
        "         ('REDUCTIONS_DES_DIMENSIONS', umap.UMAP(n_neighbors=30, min_dist=0.1,n_components=2, random_state=42) , X.select_dtypes(include=['int64','float64',\"bool\"]).columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model_UMAP = make_pipeline(\n",
        "    preprocessor_UMAP ,\n",
        "    SVC()\n",
        ")\n",
        "\n",
        "model_UMAP.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2enBq2ciMnye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluer ses performances, en général et en fonction de la valeur réelle de la variable cible**"
      ],
      "metadata": {
        "id": "KRebofHKT7n4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour choisir le meilleur modèle parmi les 9, nous allons utiliser le critére exactutide du modèle (accuracy) et nous allons choisir le modèle avec l'exactutide le plus élevé."
      ],
      "metadata": {
        "id": "WwowIOmAftzr"
      }
    },
    {
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "models = [grid, grid_1, grid_2, grid_1_PCA, grid_1_PCA, grid_2_PCA, grid_1_UMAP, grid_UMAP, grid_2UMAP]\n",
        "\n",
        "\n",
        "prediction = grid.predict(X_test)\n",
        "prediction_1 = grid_1.predict(X_test)\n",
        "prediction_1grid_2 = grid_2.predict(X_test)\n",
        "predictiongrid_1_PCA = grid_1_PCA.predict(X_test)\n",
        "prediction_1_PCA1 = grid_1_PCA.predict(X_test)\n",
        "prediction_grid_2_PCA = grid_2_PCA.predict(X_test)\n",
        "prediction_1_grid_1_UMAP = grid_1_UMAP.predict(X_test)\n",
        "prediction_1_gri = grid_UMAP.predict(X_test)\n",
        "prediction_1_gr = grid_2UMAP.predict(X_test)\n",
        "\n",
        "predictions = [prediction, prediction_1, prediction_1grid_2, predictiongrid_1_PCA, prediction_1_PCA1, prediction_grid_2_PCA, prediction_1_grid_1_UMAP, prediction_1_gri, prediction_1_gr]\n",
        "\n",
        "\n",
        "accuracies = []\n",
        "for model_predictions in predictions:\n",
        "    accuracy = accuracy_score(y_test, model_predictions)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "best_model_index = np.argmax(accuracies)\n",
        "best_model = models[best_model_index]\n",
        "best_accuracy = accuracies[best_model_index]\n",
        "\n",
        "print(f\" Le meilleur modèle est {models[best_model_index + 1]} avec l'exactutide de  {best_accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Ea3nQVdvhULf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Matrice de Confusion**"
      ],
      "metadata": {
        "id": "F3kwBxH8ZmUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "prediction_1grid_2=grid.predict(X_test)\n",
        "ConfusionMatrixDisplay(confusion_matrix(y_test,prediction_1grid_2)).plot()"
      ],
      "metadata": {
        "id": "LbG073SqWWxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interprétation des Résultats**\n",
        "\n",
        "- Vrai Négatif (TN): Il y a 4082 instances où le modèle a correctement prédit la classe négative (0).\n",
        "\n",
        "- Faux Positif (FP): Il y a 1986 instances où le modèle a prédit la classe positive (1), alors que l'instance était réellement de la classe négative (0).\n",
        "\n",
        "- Faux Négatif (FN): Il y a 1987 instances où le modèle a prédit la classe négative (0), alors que l'instance était réellement de la classe positive (1).\n",
        "\n",
        "- Vrai Positif (TP): Il y a 3839 instances où le modèle a correctement prédit la classe positive (1)."
      ],
      "metadata": {
        "id": "zJB9SG0dZ2xA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rapport de classification**"
      ],
      "metadata": {
        "id": "HL5MeOsPaGfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, prediction_1grid_2))"
      ],
      "metadata": {
        "id": "FeD3mqLPZ-Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle présente des performances similaires pour les deux classes, avec une légère différence entre les métriques de précision, rappel et score F1. La précision globale de 67% indique que le modèle a correctement classé 67 % des instances du jeu de test."
      ],
      "metadata": {
        "id": "iNFdl9ZKj-R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Sélection et influence des variables**"
      ],
      "metadata": {
        "id": "nVXYsFk3cIBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **un modèle linéaire pénalisé avec sélection par stabilité**\n"
      ],
      "metadata": {
        "id": "TrdMeMhJdx5K"
      }
    },
    {
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "preprocessor_= ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('MISE A L ECHELLE', RobustScaler(), X.select_dtypes(include=['int64', 'float64']).columns),\n",
        "        ('BINAIRE', FunctionTransformer(binary_transformer, validate=False), X.select_dtypes(include=['bool']).columns),\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "modele = make_pipeline(\n",
        "    preprocessor_,\n",
        "    LogisticRegression ()\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'logisticregression__penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'logisticregression__solver': ['liblinear', 'saga'],\n",
        "    'logisticregression__max_iter': [100]\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "Grid= GridSearchCV(modele, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "Grid.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EBT-PjjWkCVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predict=Grid.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, Predict)"
      ],
      "metadata": {
        "id": "YQloETH2i17r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Grid.best_estimator_.named_steps['logisticregression'].coef_"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "93-aEB8hss_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "coefficients = Grid.best_estimator_.named_steps['logisticregression'].coef_[0]\n",
        "\n",
        "\n",
        "feature_names = X.columns\n",
        "\n",
        "\n",
        "zero_coefficient_variables = []\n",
        "\n",
        "for i, coefficient in enumerate(coefficients):\n",
        "    if coefficient == 0:\n",
        "        zero_coefficient_variables.append(feature_names[i])\n",
        "\n",
        "print(\"Variables sélectionnées:\", zero_coefficient_variables)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GmcKt49lwzWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **une sélection par seuil sur la corrélation des variables avec la cible, suivi d’une méthode de prédiction à choisir**"
      ],
      "metadata": {
        "id": "Yakzu5GIzqj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons calculer d'abord les variances des variables quantitatives explicatives et les corrélations entre la variable cible et ses variables quantitatives explicatives.\n",
        "\n",
        "# **Remarque:**\n",
        "\n",
        "- On ne peux pas calculer une corrélation  entre une variable quantitative et une variable binaire.\n",
        "\n",
        "- On ne peux pas calculer une variance sur une variable binaire."
      ],
      "metadata": {
        "id": "PClgSQt6O2eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.concat([variables_quantitative,variables_dicretes],axis=1)\n",
        "varlist = X.var()\n",
        "data_corr=df.corr()[\" shares\"]"
      ],
      "metadata": {
        "id": "01-3oBMWzuRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La séléction des variables sera basé sur les variables dont:\n",
        "\n",
        "- La variance est supésieur ou égale à la moyenne des variances des variables quantitatives explicatives.\n",
        "\n",
        "- La corrélation   entre la variable cible et les variables quantitatives explicatives est supérieur ou égale à 0,01."
      ],
      "metadata": {
        "id": "xbY_ehdgRI3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_thr=varlist.median()\n",
        "cor_thr=0.01\n",
        "\n",
        "tokeep = []\n",
        "for i in range(df.shape[1]):\n",
        "    if varlist[i]>=var_thr and abs(data_corr[i])>=cor_thr:\n",
        "        tokeep.append(df.columns[i])\n",
        "tokeep"
      ],
      "metadata": {
        "id": "x89NI0tJ1a2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons mettre ensemble les variables obtenues par la séléction des variables et les variables qualitatives"
      ],
      "metadata": {
        "id": "HwtTjq_HVIaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Régression logistique**"
      ],
      "metadata": {
        "id": "vw04vV4McOz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous choisisons la régression logistique pour classer si un article a été populaire ou pas. les variables quantitatives utilisées sera ceux obtenues par la sélection par seuil sur la corrélation des variables avec la cible et les variables bainaires restantes."
      ],
      "metadata": {
        "id": "A4n50dPXciyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEF=pd.concat([data[tokeep],variables_qualitative],axis=1)\n",
        "\n",
        "x_=DEF\n",
        "y_=data[\"Popularite\"]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_, y_, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "QUVPaRe-6zxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "\n",
        "\n",
        "def binary_transformer(x):\n",
        "    return x.astype(int)\n",
        "\n",
        "\n",
        "preprocessor_= ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('MISE A L ECHELLE', RobustScaler(), x.select_dtypes(include=['int64', 'float64']).columns),\n",
        "        ('BINAIRE', FunctionTransformer(binary_transformer, validate=False), x.select_dtypes(include=['bool']).columns),\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "modele = make_pipeline(\n",
        "    preprocessor_,\n",
        "    LogisticRegression ()\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'logisticregression__penalty': ['l1'],\n",
        "    'logisticregression__solver': ['liblinear', 'saga'],\n",
        "    'logisticregression__max_iter': [100]\n",
        "\n",
        "}\n",
        "\n",
        "Grid= GridSearchCV(modele, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "Grid.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Xdb-9G6m9O1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Grid.best_estimator_.named_steps['logisticregression'].coef_"
      ],
      "metadata": {
        "id": "1sqVjp5S9fOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "HykZY9NQmoCZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}